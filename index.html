<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>KMNIST Character Classification — Group 10</title>
  <meta name="description" content="KMNIST Character Classification: logistic regression, CNNs, few-shot and Siamese networks. Project by Group 10, Texas A&M." />
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    body { padding-top: 4.5rem; }
    .hero { background: linear-gradient(90deg,#0d6efd1a,#6f42c11a); padding: 4rem 0; }
    .card-figure { max-height: 280px; object-fit: contain; }
    pre.codebox { background:#0f1720; color:#d1d5db; padding:1rem; border-radius:.5rem; overflow:auto; }
    footer { padding: 3rem 0; color:#6c757d; }
    .tag { font-size:.85rem; padding:.25rem .5rem; border-radius:.4rem; background:#eef2ff; color:#3730a3; margin-right:.4rem;}
  </style>
</head>
<body>
<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
  <div class="container">
    <a class="navbar-brand" href="#">KMNIST Project — Group 10</a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navmenu">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div id="navmenu" class="collapse navbar-collapse">
      <ul class="navbar-nav ms-auto">
        <li class="nav-item"><a class="nav-link" href="#overview">Overview</a></li>
        <li class="nav-item"><a class="nav-link" href="#methodology">Methodology</a></li>
        <li class="nav-item"><a class="nav-link" href="#experiments">Experiments</a></li>
        <li class="nav-item"><a class="nav-link" href="#results">Results</a></li>
        <li class="nav-item"><a class="nav-link" href="#fewshot">Few-Shot</a></li>
        <li class="nav-item"><a class="nav-link" href="#resources">Resources</a></li>
      </ul>
    </div>
  </div>
</nav>

<header class="hero">
  <div class="container">
    <div class="row align-items-center">
      <div class="col-lg-8">
        <h1 class="display-5">KMNIST Character Classification</h1>
        <p class="lead">From a logistic regression baseline to tuned CNNs and few-shot Siamese networks — this project evaluates architectures, tuning, and interpretability on Kuzushiji-MNIST.</p>
        <p>
          <span class="tag">KMNIST</span>
          <span class="tag">CNN</span>
          <span class="tag">Few-Shot</span>
          <span class="tag">Siamese Network</span>
        </p>
        <p>
          <a class="btn btn-primary btn-lg" href="/mnt/data/ECEN_data_mining_and_analysis_project.pdf" role="button" target="_blank">Download Full Report (PDF)</a>
          <a class="btn btn-outline-secondary btn-lg" href="#resources">View Code & Blog</a>
        </p>
      </div>
      <!-- <div class="col-lg-4 d-none d-lg-block">
        <div class="card shadow-sm">
          <img src="assets/images/fig_pca_placeholder.png" alt="PCA scatter placeholder" class="card-img-top card-figure">
          <div class="card-body">
            <h6 class="card-title">PCA of KMNIST (placeholder)</h6>
            <p class="card-text small text-muted">Replace this image with <code>fig_pca.png</code> exported from the PDF.</p>
          </div>
        </div>
      </div> -->
    </div>
  </div>
</header>

<main class="container my-5">
  <section id="overview" class="mb-5">
    <div class="row">
      <div class="col-lg-8">
        <h2>Overview</h2>
        <p>The Kuzushiji-MNIST (KMNIST) dataset contains 70,000 28×28 grayscale images of cursive Japanese characters across 10 classes. This work explores the performance gap between linear models and convolutional approaches, the effect of regularization and residual connections, and studies few-shot/metric-learning techniques to operate in low-data regimes.</p>

        <h4>Project goals</h4>
        <ul>
          <li>Establish a logistic regression baseline on flattened pixels.</li>
          <li>Implement and compare three CNN variants: Simple CNN, ResNet-like, and Improved CNN with batch norm + dropout.</li>
          <li>Perform validation-driven hyperparameter tuning to produce a TunedImprovedCNN.</li>
          <li>Analyze failure cases and saliency maps for interpretability.</li>
          <li>Explore few-shot supervised learning and Siamese metric learning for one-shot classification.</li>
        </ul>
      </div>
      <div class="col-lg-4">
        <div class="card mb-3">
          <div class="card-body">
            <h5 class="card-title">Team</h5>
            <p class="mb-0"><strong>Noureddin Lutfi</strong><br /><small>noorlutfe@tamu.edu</small></p>
            <p class="mb-0"><strong>Aditya Rajiv</strong><br /><small>adiman@tamu.edu</small></p>
            <p class="mb-0"><strong>Nuo Chen</strong><br /><small>nuochen@tamu.edu</small></p>
            <p class="mb-0"><strong>Diviya Bhavaani</strong><br /><small>diviyabhavaani@tamu.edu</small></p>
            <hr>
            <a href="https://github.com/adimantamu/ECEN_Data_Mining_Project_Group10" target="_blank">GitHub Repository</a><br>
            <!-- <a href="https://diviyabhavaani-mb.github.io/KMNIST_GROUP_10_Project_Blog/" target="_blank">Project Blog Post</a> -->
          </div>
        </div>
        <!-- <div class="alert alert-info small">
          Want this site as a Jekyll theme? Put this file in a Jekyll-enabled repo and add the front matter — I can prepare that too.
        </div> -->
      </div>
    </div>
  </section>

  <section id="methodology" class="mb-5">
    <h2>Methodology</h2>
    <p>This section describes dataset preprocessing, model architectures, training procedures, and hyperparameter tuning.</p>

    <h4>Dataset & preprocessing</h4>
    <p>KMNIST images are normalized to [0,1] and reshaped for CNN input. For model selection we used an 80/20 train/validation split of the original 60,000 training images (48k train, 12k validation). Data augmentation was not the main focus for the baseline experiments but is recommended for future work (rotations, slight affine transforms, intensity jitter).</p>

    <h4>Baseline: Multinomial Logistic Regression</h4>
    <p>We flattened each 28×28 image into a 784-dimensional vector, standardized features using <code>StandardScaler</code>, and trained an sklearn multinomial logistic regression on a random subset of 10,000 training samples to get a quick baseline.</p>

    <h4>Model architectures (high level)</h4>
    <ol>
      <li><strong>Simple CNN</strong> — 2 convolutional layers (32→64), ReLU, 2×2 max-pool, FC to 128 then to 10 classes. Trained with Adam (LR=1e-3) for 5 epochs.</li>
      <li><strong>ResNet-like CNN</strong> — 2 residual blocks (skip connections), batch pooling, FC head. Trained for 10 epochs.</li>
      <li><strong>Improved CNN</strong> — deeper stack of residual-like blocks, BatchNorm after convolutions, dropout in FC layer (0.4). Trained with Adam.</li>
    </ol>

    <h4>Hyperparameter Tuning</h4>
    <p>A grid search validated learning rate, dropout, and batch size. The grid: LR ∈ {0.001, 0.0005}, dropout ∈ {0.3,0.4,0.5}, batch size ∈ {64,128}. Each candidate was trained for 3 epochs during the search, and the best was retrained for 10 epochs.</p>
  </section>

  <section id="experiments" class="mb-5">
    <h2>Experiments & Training Details</h2>
    <div class="row">
      <div class="col-md-6">
        <h5>Training choices</h5>
        <ul>
          <li>Loss: Cross-entropy (multi-class)</li>
          <li>Optimizer: Adam</li>
          <li>Normalization: BatchNorm for deeper networks</li>
          <li>Regularization: Dropout in fully connected layers</li>
          <li>Validation monitoring: choose best epoch by validation accuracy</li>
        </ul>
        <h5>Compute & reproducibility</h5>
        <p>Experiments were run on a single GPU when available; random seeds were fixed for reproducibility. All notebooks and training scripts are available in the repository linked above.</p>
      </div>

      <div class="col-md-6">
        <h5>Few-Shot Protocol</h5>
        <p>For supervised few-shot: sample K ∈ {1,5,10} images per class (fixed seed), train the Simple CNN for 15 epochs, evaluate on the full test set. For metric learning: train a Siamese CNN encoder with contrastive loss; evaluate in 10-way one-shot using a single support image per class (prototypes).</p>
      </div>
    </div>

    <hr>

    <h5>Code snippet (PyTorch-style training loop overview)</h5>
    <pre class="codebox"><code># simplified; see repo for full code
for epoch in range(epochs):
    model.train()
    for X,y in train_loader:
        pred = model(X)
        loss = criterion(pred, y)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    val_acc = evaluate(model, val_loader)
    if val_acc > best:
        save_checkpoint(model)</code></pre>
  </section>

  <section id="results" class="mb-5">
    <h2>Results & Analysis</h2>
    <p>Summary of test performance across models (full table in the paper):</p>

    <div class="table-responsive mb-3">
      <table class="table table-striped">
        <thead>
          <tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1</th></tr>
        </thead>
        <tbody>
          <tr><td>Logistic Regression</td><td>0.598</td><td>0.609</td><td>0.598</td><td>0.600</td></tr>
          <tr><td>Simple CNN</td><td>0.934</td><td>0.935</td><td>0.934</td><td>0.934</td></tr>
          <tr><td>ResNet-like CNN</td><td>0.943</td><td>0.943</td><td>0.943</td><td>0.943</td></tr>
          <tr><td>Improved CNN</td><td>0.967</td><td>0.968</td><td>0.967</td><td>0.967</td></tr>
          <tr><td>TunedImprovedCNN</td><td><strong>0.970</strong></td><td>0.970</td><td>0.970</td><td>0.970</td></tr>
        </tbody>
      </table>
    </div>

    <h5>Error analysis</h5>
    <p>Confusion matrices indicate most remaining errors are visually ambiguous characters (faint strokes, missing components). Saliency maps corroborate that the model focuses on the main strokes; errors happen when discriminative stroke elements are missing or noisy.</p>

    <!-- <h5>Figures (replace placeholders)</h5>
    <div class="row">
      <div class="col-md-6 mb-4">
        <div class="card">
          <img class="card-img-top card-figure" src="assets/images/fig_confusion_simple.png" alt="Confusion Simple">
          <div class="card-body">
            <p class="card-text">Confusion matrix — Simple CNN. <em>Replace with actual exported image.</em></p>
          </div>
        </div>
      </div>
      <div class="col-md-6 mb-4">
        <div class="card">
          <img class="card-img-top card-figure" src="assets/images/fig_confusion_improved.png" alt="Confusion Improved">
          <div class="card-body">
            <p class="card-text">Confusion matrix — Improved CNN.</p>
          </div>
        </div>
      </div>
    </div> -->
  </section>

  <section id="insights" class="mb-5">
    <h2>Business Insights</h2>
    <p>Based on our experimental results and misclassification analysis, several insights emerge for real-world applications of KMNIST-like character recognition systems.</p>
    
    <ul>
      <li><strong>High-accuracy CNN models (≥97%) are suitable for production deployment</strong> where reading historical or handwritten scripts is required—such as digitizing Japanese archives or automating documentation workflows.</li>
      <li><strong>Error patterns indicate specific characters that need specialized augmentation</strong> due to visual similarity. Targeted augmentation or class-specific tuning can improve these weak spots.</li>
      <li><strong>Few-shot and Siamese networks enable low-data deployment</strong> in scenarios where annotated samples are expensive to collect (e.g., rare symbols, historical variants).</li>
      <li><strong>Interpretability tools (e.g., saliency maps) show which strokes contribute to predictions</strong>, supporting real-world audits of model reliability.</li>
      <li><strong>Model size vs. accuracy trade-offs</strong> suggest the Improved CNN is optimal for edge deployment due to high accuracy with moderate complexity.</li>
    </ul>
  </section>
  
  <section id="conclusion" class="mb-5">
    <h2>Conclusion</h2>
    <p>This project demonstrated that convolutional neural networks dramatically outperform linear baselines on KMNIST, with our TunedImprovedCNN achieving a state-of-the-art accuracy of 97%. Through error analysis and visualization, we identified the model’s strengths and limitations.</p>
  
    <p>Further, the few-shot and Siamese experiments showed that metric-learning approaches significantly enhance performance when labeled data is limited, achieving 77.67% in one-shot classification.</p>
  
    <p>Overall, our study highlights the importance of careful architecture design, hyperparameter tuning, and interpretability techniques in building reliable character recognition systems for real-world use.</p>
  </section>

  <!-- ================= Gallery Section ================= -->
  <section id="gallery" class="mb-5">
    <h2>Project Figures & Images</h2>
    <p>All relevant figures and visuals from our KMNIST experiments:</p>
  
    <div id="gallery-container" class="row row-cols-1 row-cols-sm-2 row-cols-md-3 g-4">
      <!-- Gallery cards will be injected here by JavaScript -->
    </div>
  </section>
  
  <!-- ================= Gallery Script ================= -->
  <script>
    // List all image filenames in the 'images/' folder (update this list!)
    const imageFiles = [
      "Class distribution in training set.png",
      "ResNet-like-CNN - Test Confusion Matrix.png",
      "Confusion Matrix – Logistic Regression.png",
      "Simple CNN Test Confusion Matrix.png",
      "Improved CNN - Test Confusion Matrix.png",
      "Tuned Improved CNN - Test Confusion Matrix.png",
      "Misclassified examples for TunedImprovedCNN.png",
      "Tuned Improved CNN - Train Confusion Matrix.png",
      "PCA visualization of KMNIST.png"
    ];
    
    const galleryContainer = document.getElementById('gallery-container');
  
    imageFiles.forEach(filename => {
      // Create column
      const col = document.createElement('div');
      col.className = 'col';
  
      // Create card
      const card = document.createElement('div');
      card.className = 'card h-100';
  
      // Create image
      const img = document.createElement('img');
      img.src = `images/${filename}`;
      img.alt = filename.split('.')[0].replace(/_/g, ' ');
      img.className = 'card-img-top card-figure';
  
      // Create card body
      const cardBody = document.createElement('div');
      cardBody.className = 'card-body';
  
      // Caption
      const caption = document.createElement('p');
      caption.className = 'card-text';
      caption.textContent = filename.split('.')[0].replace(/_/g, ' ');
  
      // Assemble card
      cardBody.appendChild(caption);
      card.appendChild(img);
      card.appendChild(cardBody);
      col.appendChild(card);
  
      // Append to gallery container
      galleryContainer.appendChild(col);
    });
  </script>

  <section id="fewshot" class="mb-5">
    <h2>Research Extension — Few-Shot & Metric Learning</h2>
    <p>We evaluated the Simple CNN in severely limited-data regimes and implemented a Siamese network for metric learning.</p>

    <h5>Few-shot supervised baseline</h5>
    <p>With only K labeled samples per class the Simple CNN obtains:</p>
    <ul>
      <li>K=1 → 32.21% test accuracy</li>
      <li>K=5 → 45.08% test accuracy</li>
      <li>K=10 → 53.34% test accuracy</li>
    </ul>

    <h5>Siamese network</h5>
    <p>A Siamese convolutional network trained with contrastive loss produced a 256-D embedding. In 10-way one-shot evaluation using nearest-prototype classification, it reached <strong>77.67%</strong> accuracy — a large improvement for extreme low-data scenarios.</p>

    <blockquote class="blockquote">Practical note: metric learning is often more robust than naïve supervised training in one-shot/low-shot settings because it optimizes for instance similarity rather than mapping inputs directly to labels.</blockquote>
  </section>

  <section id="resources" class="mb-5">
    <h2>Resources</h2>
    <ul>
      <li><strong>Repo:</strong> <a href="https://github.com/adimantamu/ECEN_Data_Mining_Project_Group10" target="_blank">adimantamu/ECEN_Data_Mining_Project_Group10</a></li>
      <li><strong>Project blog:</strong> <a href="https://diviyabhavaani-mb.github.io/KMNIST_GROUP_10_Project_Blog/" target="_blank">KMNIST Group 10 Blog</a></li>
      <li><strong>Full report (PDF):</strong> <a href="/mnt/data/ECEN_data_mining_and_analysis_project.pdf" target="_blank">Download PDF</a></li>
    </ul>

    <h5>How to reproduce</h5>
    <p>See the repository for Jupyter notebooks that include training loops, model definitions, grid search code, and scripts to generate the figures in this site.</p>

    <h5>How to use the images in this HTML</h5>
    <ol>
      <li>Extract images from the PDF (see detailed instructions below).</li>
      <li>Place extracted images in <code>assets/images/</code> inside your repo.</li>
      <li>Ensure the filenames match the <code>src</code> values in this HTML (or update the paths).</li>
    </ol>
  </section>

  <section class="mb-5">
    <h3>Deployment & next steps</h3>
    <p>Once images are added and the file is committed:</p>
    <ol>
      <li>Push to GitHub: <code>git add . &amp;&amp; git commit -m "Update site" &amp;&amp; git push</code></li>
      <li>Enable GitHub Pages in repo Settings → Pages (branch: <code>main</code> or <code>gh-pages</code>)</li>
      <li>Your site will be live at <code>&lt;username&gt;.github.io/&lt;repo&gt;</code> or <code>&lt;username&gt;.github.io</code> if the repo is named <code>&lt;username&gt;.github.io</code>.</li>
    </ol>
  </section>

  <hr>

  <section class="mb-5">
    <h4>Want me to:</h4>
    <ul>
      <li>Automatically extract images from your PDF and update the HTML image tags? (I can show commands you can run locally.)</li>
      <li>Convert this into a Jekyll-ready site with navigation and blog posts? (I can add front matter.)</li>
      <li>Generate a smaller, mobile-first layout or a printable version? (Say which.)</li>
    </ul>
  </section>

</main>

<footer class="bg-light">
  <div class="container">
    <div class="row">
      <div class="col-md-8">
        <p><strong>KMNIST Character Classification — Group 10</strong> — Texas A&M University</p>
        <p class="small">Authors: Noureddin Lutfi, Aditya Rajiv, Nuo Chen, Diviya Bhavaani. For full technical details, figures, and notebooks, download the report or visit the GitHub repository.</p>
      </div>
      <div class="col-md-4 text-md-end">
        <p class="small">© 2025 Group 10</p>
      </div>
    </div>
  </div>
</footer>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
